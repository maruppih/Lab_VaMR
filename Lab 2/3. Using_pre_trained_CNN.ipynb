{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l5xxcxd0vpW"
      },
      "source": [
        "# Using pre-trained CNN\n",
        "\n",
        "Usare modelli già addestrati da altri\n",
        "\n",
        "In this lab, we will see:\n",
        "\n",
        "- Zero-shot performance of pre-trained backbone, perfomance del modello senza fare nulla, si prende un modello e si testa. Se non addestro il modello per nulla sui dati, darò poche performance\n",
        "- Use pre-trained CNN as backbone, utilizzarlo come backbone del modello, spesso un modello viene suddiviso in due parti: una backbone (corpo, che può essere CNN, trasformer...) e un classifier (head). Si va ad addestrare solo la testa con un modello pre-trained (la testa sarà inizializzata random), quando si hanno pochi dati (uso modello già pre-trained), avrò performance buone ma non quanto quelle di finetuning. Qui la backbone è frozen. Se addestro solo la testa, avrò performance migliori rispetto a prima\n",
        "- Fine-tuning the pre-trained CNN, aggiorna anche i pesi della backbone, si spostano anche i pesi della CNN, la testa si adatta sui dati che abbiamo e anche i pesi della rete. Questo porta beneficio. Non ci dimentichiamo del passato, non dobbiamo sciupare performance modelli pre-trained. I modelli pretrained sono stati addestrati su dataset molto grossi, con un numero di immagini molto elevato rispetto a quelle che abbiamo in locale. Vogliamo adattare modello per apprendere i dati che abbiamo, ma senza sciupare più di tanto i dati che conosceva e su cui era stato appreso. Se sposto un pochino anche la mia backbone, avrò performance migliori\n",
        "\n",
        "(punti ordinati in base alla facilità di implementazione e alle performance)\n",
        "\n",
        "\n",
        "!!PARAGONE CIFAR18 RESNET VS QUESTA RETE (3 punti)!! pre-trained model funziona meglio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tkm5SfEo1c1r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Oh2yVOi3Lie"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "lr = 0.01 # 0.01  #0.1\n",
        "epochs = 10\n",
        "device = torch.device(\"cuda\") # to use the GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWJUU9yp0vHp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14d107ed-af04-4969-d655-dc7021e43197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# usiamo sempre CIFAR10, import dei dati\n",
        "# quello che vediamo è usare dei modelli pre-trained model su ImageNet e lo lanciamo su CIFAR10 e vediamo quanto migliorano le performance\n",
        "# con CIFAR10 dell'altra volta si riusciva ad arrivare a delle performance circa del 60%, vediamo se con queste tre opzioni riusciamo a migliorare (sì)\n",
        "\n",
        "# RandomCrop la croppo 32x32 in modo randomico, prima di fare il crop viene fatto il padding con dei valori neri (quindi a 0), l'immagine diventa 36 x 36 (perché CIFAR10 di partenza è 32 x 32)\n",
        "# quindi l'immagine sarà un po' spostata\n",
        "# RandomHorizontalFlip, prob = 1/2  (0.5) di fare flipping (di base), rovescia a speccio\n",
        "\n",
        "# quando si fa il training\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "\n",
        "# non faccio nessuna trasformazione a test time\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "\n",
        "# create a split for train/validation. We can use early stop, utile per questo, ovvero vuol dire scegliere il modello migliore che vogliamo portarci dietro sulla base del validation set\n",
        "# l'idea è questa: quando si fa un training molto lungo con tante epoche, ci sarà un certo momento in cui avrò performance migliori e poi andando avanti si andrà a peggiorare\n",
        "# quindi quando avrò ottenuto la performance migliore voglio salvarmi quello stato e poi utilizzarlo per fare il test. Questo è Early Stopping, ma non dobbiamo usare il test set, ma il val set\n",
        "# il test set va usato solo a fine apprendimento, sennò è barare!!\n",
        "\n",
        "\n",
        "trainset, valset = torch.utils.data.random_split(dataset, [40000, 10000]) # dataset, numero immagini train 40000 e validation 10000, in uscita avrò due dataset\n",
        "\n",
        "\n",
        "# costruisco i dataloader\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2,\n",
        "                                          drop_last=True)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2,\n",
        "                                          drop_last=False)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2,\n",
        "                                          drop_last=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load a pre-defined network with pretrained weights\n",
        "come si prendono le reti pre-addestrate da pytorch?\n",
        "\n"
      ],
      "metadata": {
        "id": "mkiBrlvS1utb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dalla libreria torchvision.models ci sono definite tutte le architetture\n",
        "# si definiscono tutte come sotto, prendono pretrained in ingresso se è = TRUE ci dà modello con i pesi pre-addestrati su ImageNet\n",
        "\n",
        "net = models.resnet18(pretrained=True)\n",
        "net\n",
        "\n",
        "# PROBLEMA: RETE STESSA!\n",
        "# problema: se io apprendo dei pesi sulle immagini 24 x 24 (imagenet), le posso usare su immagini su 32 x 32 (CIFAR)\n",
        "# problema: imagenet ha 1000 classi (out_features), CIFAR10 ne ha 10, quello che viene fatto è buttare via l'ultimo livello della rete (fc, cioè il classifier) e andare a rinizializzarlo con un classificatore con 10 classi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6igOo5t7to7y",
        "outputId": "4c850880-969a-417f-e300-a4f5b3c196b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:01<00:00, 40.6MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uav1QxKwUXE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d278b5ee-f3d2-4349-fc9c-0565cb08cd07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "\n",
        "\n",
        "net = models.resnet18(pretrained=True) # pretrained = True ci inizializza il modello con i pesi pre-addestrati su imagenet\n",
        "\n",
        "\n",
        "# per sovrascrivere il classificatore quello che si fa si fa un override e si fa uscire con una dimensione = 10\n",
        "\n",
        "# override the fc layer of the network since it is of 1000 classes by default (ImageNet)\n",
        "net.fc = nn.Linear(512, 10) # si fa override layer fc, si fa uscire con una uscita a 10 invece che a 1000, il fc così però avrà dei valori randomici, si vede da \"out_features\"\n",
        "# quindi si perdere il classificatore addestrato su ImageNet, ma abbiamo a questo punto una rete pre-addestrata su ImageNet con l'unica eccezione che il fc finale avrà pesi randomici\n",
        "\n",
        "net.to(device)\n",
        "\n",
        "# 2048 resnet50 al posto di 512"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ogni rete può essere presa nei suoi layer intermedi attraverso il ."
      ],
      "metadata": {
        "id": "_VfmS7bXqweb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# posso accedere a qualsiasi livello\n",
        "\n",
        "net.conv1\n",
        "\n",
        "#e ai pesi\n",
        "\n",
        "#net.conv1.weight"
      ],
      "metadata": {
        "id": "1K-u_yM-u1D3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5971d397-f0fa-4d21-9ed6-7b008faeba8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "la ResNet è costruita a blocchi, layer1,2,3,4\n",
        "\n",
        "Ogni singolo layer è un modulo che si chiama Sequential perché a sua volta è formato da dei blocchi che sono il blocco 0 e il blocco 1\n",
        "\n",
        "I blocchi Sequential sono una lista, si accedono così:"
      ],
      "metadata": {
        "id": "yii0v0ODrPw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net.layer1\n",
        "\n",
        "#net.layer1[0] # accedo al blocco\n",
        "\n",
        "#net.layer1[0].conv1"
      ],
      "metadata": {
        "id": "vyVW4W1qvFcZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c9c42ff-db80-4c41-ce34-0bdee81b400f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): BasicBlock(\n",
              "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (1): BasicBlock(\n",
              "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nella ResNet ci sono sia dei layer che dei blocchi sequenziali. I blocchi sequenziali sono questi che chiamano layer1,2,3,4 mentre i layer normali sono quelli che hanno come keyword avgpooling, fc, conv, bn, relu e maxpool (dal basso verso l'alto)"
      ],
      "metadata": {
        "id": "X8YC25d8slua"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWSNZ5Hf18Kn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daf10f39-154b-404d-ef10-6a74ceb00f58"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# count the trainable parameters of the model\n",
        "# conta il numero di parametri che sono addestrabili, comodo per vedere quanti layer sto addestrando e quanti li sto freezando\n",
        "\n",
        "\n",
        "def count_trainable_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad) # requires_grad mi dice se è freezato o learnable, TRUE deve essere appreso, FALSE no, ritorna la somma dei parametri che richiedono gradiente\n",
        "count_trainable_parameters(net)\n",
        "\n",
        "# la ResNet18 ha 11 milioni di parametri (11181642)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "vogliamo agire sui paramtetri del modello, ne fissiamo alcuni mentre learnable degli altri"
      ],
      "metadata": {
        "id": "_YUKTARHtwG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# frozen all the weights of the network, except for fc ones\n",
        "# fissiamo alcuni e altri li mettiamo learnable\n",
        "for param in net.parameters():\n",
        "    param.requires_grad = False # mettiamo a false TUTTI i parametri della rete # se non facessimo altro la rete sarebbe non apprendibile\n",
        "#net.fc.weight.requires_grad = True # metto i pesi dei rg a true nel fc, quaindi al classificatore, sia al peso che al bias # FROZEN\n",
        "#net.fc.bias.requires_grad = True # e anche al bias del classificatore, l'ultimo layer prende un input y = x*W^T + b, input x matrice dei pesi + bias, teniamo freezati tutti i pesi tranne W e b # FROZEN\n",
        "#count_trainable_parameters(net)\n",
        "\n",
        "# ho un modello grosso di 11 milioni di parametri, ma ne addestro solo 5000"
      ],
      "metadata": {
        "id": "3uNOIDDs1_QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "quindi: ho fatto override sul classificatore inizializzato a random, voglio addestrarlo sui miei (pochi) dati può essere funzionale perché ho un modello grosso con 11 milioni di parametri, ma di questi ne addestro solo 5000"
      ],
      "metadata": {
        "id": "AKZt5Peku_ld"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UH7STI6m0dg9"
      },
      "outputs": [],
      "source": [
        "# define train and test function\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 500 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "        losses.append(loss.item())\n",
        "    return np.mean(losses)\n",
        "\n",
        "def test(model, device, test_loader, val=False):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    mode = \"Val\" if val else \"Test\"\n",
        "    print('\\{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        mode,\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    test_acc = correct / len(test_loader.dataset)\n",
        "    return test_loss, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=1e-04)"
      ],
      "metadata": {
        "id": "b3SqSMwo4qp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqA_UXQZ2zc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78bbad5c-b9c2-49cf-dc9a-ee1ebc14e6f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.584742\n",
            "Train Epoch: 1 [32000/40000 (80%)]\tLoss: 2.453959\n",
            "\\Test set: Average loss: 412.9323, Accuracy: 984/10000 (10%)\n",
            "\n",
            "Train Epoch: 2 [0/40000 (0%)]\tLoss: 2.749192\n",
            "Train Epoch: 2 [32000/40000 (80%)]\tLoss: 2.496544\n",
            "\\Test set: Average loss: 412.3418, Accuracy: 952/10000 (10%)\n",
            "\n",
            "Train Epoch: 3 [0/40000 (0%)]\tLoss: 2.547477\n",
            "Train Epoch: 3 [32000/40000 (80%)]\tLoss: 2.505162\n",
            "\\Test set: Average loss: 407.9046, Accuracy: 971/10000 (10%)\n",
            "\n",
            "Train Epoch: 4 [0/40000 (0%)]\tLoss: 2.740646\n",
            "Train Epoch: 4 [32000/40000 (80%)]\tLoss: 2.701255\n",
            "\\Test set: Average loss: 409.3044, Accuracy: 989/10000 (10%)\n",
            "\n",
            "Train Epoch: 5 [0/40000 (0%)]\tLoss: 2.583096\n",
            "Train Epoch: 5 [32000/40000 (80%)]\tLoss: 2.701384\n",
            "\\Test set: Average loss: 411.3036, Accuracy: 939/10000 (9%)\n",
            "\n",
            "Train Epoch: 6 [0/40000 (0%)]\tLoss: 2.594532\n",
            "Train Epoch: 6 [32000/40000 (80%)]\tLoss: 2.651295\n",
            "\\Test set: Average loss: 414.4543, Accuracy: 951/10000 (10%)\n",
            "\n",
            "Train Epoch: 7 [0/40000 (0%)]\tLoss: 2.501345\n",
            "Train Epoch: 7 [32000/40000 (80%)]\tLoss: 2.603162\n",
            "\\Test set: Average loss: 412.6295, Accuracy: 1028/10000 (10%)\n",
            "\n",
            "Train Epoch: 8 [0/40000 (0%)]\tLoss: 2.705131\n",
            "Train Epoch: 8 [32000/40000 (80%)]\tLoss: 2.639007\n",
            "\\Test set: Average loss: 413.1101, Accuracy: 965/10000 (10%)\n",
            "\n",
            "Train Epoch: 9 [0/40000 (0%)]\tLoss: 2.484124\n",
            "Train Epoch: 9 [32000/40000 (80%)]\tLoss: 2.463945\n",
            "\\Test set: Average loss: 410.4227, Accuracy: 965/10000 (10%)\n",
            "\n",
            "Train Epoch: 10 [0/40000 (0%)]\tLoss: 2.714414\n",
            "Train Epoch: 10 [32000/40000 (80%)]\tLoss: 2.479229\n",
            "\\Test set: Average loss: 412.9676, Accuracy: 987/10000 (10%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# the main loop\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "model_state_dict = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train(net, device, trainloader, optimizer, epoch)\n",
        "    train_losses.append(train_loss)\n",
        "    val_loss, val_acc = test(net, device, valloader)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "# perché la loss sta andando ad aumentare?\n",
        "# la backbone è completamente diversa da quella che è la testa, la testa si spostando dai dati che abbiamo di partenza, non abbiamo grosse performance con lr=0.1\n",
        "# no grande performance, migliori con 31%\n",
        "\n",
        "\n",
        "\n",
        "# vorrei loss di training (bassa), accuratezza di validation (alta), monitorare una delle due"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iwFGBFz6FUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dfbfc1f-34f4-4d34-a23f-2307f58549b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\Test set: Average loss: 340.5006, Accuracy: 3551/10000 (36%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# fare il load dei pesi sulla rete # QUI\n",
        "\n",
        "test_loss, test_acc = test(net, device, testloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "zero-shot: non eseguire cella grad, 15%, freezo tutto, prova train (non fa) rieseguo cella overraid e poi faccio train (fa)\n",
        "\n",
        "2: abbiamo freezato tutti i pesi della backbone e tenere soltanto i pesi del classificatore liberi, la loss parte bassa ma aumenta di molto"
      ],
      "metadata": {
        "id": "2mBMQqLd2ARu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add additional layer to the pre-trained model\n"
      ],
      "metadata": {
        "id": "oo5XBPHC5ePQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fc1 = nn.Linear(512, 128)\n",
        "\n",
        "# 512 era l'uscita resnet 128\n",
        "\n",
        "# Modify the existing fully connected layer (fc)\n",
        "net.fc = nn.Linear(128, 10)\n",
        "\n",
        "# poi ci metto 10\n",
        "\n",
        "# Replace the model's classifier with a new sequential layer\n",
        "# that includes the new fc1 and the modified fc\n",
        "net.fc = nn.Sequential(\n",
        "    fc1,\n",
        "    nn.ReLU(),   # Optional: Add an activation function like ReLU\n",
        "    net.fc # classificatore finale\n",
        ")\n",
        "net.to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "Z1UXzoW65drQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b04b5101-abc4-4aad-e2f7-b0ae5f503a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning some part of the CNN (not only the classifier)\n",
        "\n",
        "non spostiamo solo il fc, ma spostiamo anche qualche layer precedente\n",
        "\n",
        "ma quanti layer finetunare? quanti dei 4 layer voglio spostare? tutti? uno? due? tre?"
      ],
      "metadata": {
        "id": "otJejt0A1f9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# quanti layer andare a fine-tunare? quanti di questi layer io voglio spostare, aggiornare?\n",
        "# qui spostiamo solo il 4° blocco della rete, lasciando freezed 1 2 3\n",
        "# oltre a sbloccare il fc, il classificare, mettiamo a True i parametri del layer 4\n",
        "\n",
        "# perchè si fa così?\n",
        "\n",
        "# i layer di una rete convoluzionale sono tipicamente più o meno sempre gli stessi, imparano filtri simili, man mano che andiamo in profondità con i layer, i filtri diventano sempre più specifici\n",
        "# spesso si finetuna l'ultimo layer e poi quelli un pochino precedenti, però sempre verso la cima della rete\n",
        "# per far questo, una cosa che si può fare è andare a specializzare dei vari valori di lr sulla base di quale layer noi stiamo calcolando lo spostamento del gradiente\n",
        "\n",
        "\n",
        "for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze layer4 parameters, all'interno 4° blocco della rete\n",
        "for param in net.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Unfreeze fc layer parameters\n",
        "net.fc.requires_grad = True\n",
        "\n",
        "print(count_trainable_parameters(net))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7sOAxO0Buuz",
        "outputId": "fee3b813-5273-452f-f113-0c9e3b0ee1a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8393728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4nO6OAdXsXW"
      },
      "outputs": [],
      "source": [
        "# Setting different learning rates, si vanno a specilizzare vari valori di lr in base a dove stiamo calcolando lo spostamento del gradiente\n",
        "# specializziamo il lr in base se stiamo aggiornando il classificatore o layer convoluzionali\n",
        "# è bene spostarli, ma di poco\n",
        "# bisogna spostare poco i layer che sono già stati pre-addestrati, ecco perchè spostatiamo poco il layer 4\n",
        "# mentre dobbiamo spostare tanto, in maniera corretta, quei layer che sono stati inizializzati a random, per esempio il classificatore\n",
        "\n",
        "# lay3\n",
        "layer4_params = {'params': net.layer4.parameters(), 'lr': 0.001} # circa 7995000\n",
        "fc_params = {'params': net.fc.parameters(), 'lr': 0.1} # circa 5000\n",
        "# bisogna spostare poco quei layer che sono già stati addestrati, invce vanno spostati in maniera corretta i random\n",
        "\n",
        "\n",
        "# come si fa ad inizializzare un ottimizzatore pwe agire su questi parametri?\n",
        "# Assuming you are using an Adam optimizer\n",
        "optimizer = torch.optim.SGD([layer4_params, fc_params], momentum=0.9, weight_decay=1e-04) # 8393728\n",
        "# posso specializzare quali sono i suoi parametri, questo vuol dire che questi parametri del layer 4 verranno aggiornati con un lr =0.001 con un momentum 0.9 e con quel weight_decay\n",
        "# mentre i parametri del fc avrabbi lr = 0.1 momentum 0.9 e quel weight_decay\n",
        "\n",
        "# così si definiscono diversi lr per diversi layer\n",
        "\n",
        "\n",
        "#optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=1e-04) # invece di passargli questo...\n",
        "\n",
        "\n",
        "\n",
        "# vediamo cosa succede al training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "qui su 11 milioni di parametri ne sto spostando 8 milioni! (layer 4), è vero che stiamo spostando poco ma stiamo spostando molti paramentri"
      ],
      "metadata": {
        "id": "mNP9R6LECdvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1\n",
        "\n",
        "How many layers it is better to fine-tune?\n",
        "\n",
        "It is better to update all the weights of the model?\n",
        "\n",
        "andare a lavorare sul numero di layer di finetunare, si possono fare anche tutti, è bene spostarli un pochino tutti, risultati migliori quando li sposto tutti (risultati migliori), da pochi a tanti\n",
        "\n",
        "noi abbiamo finetunato tutto il layer 4, magari mettere learnable blocco 0 e freezed 1\n",
        "\n",
        "quaesti modelli più grandi sono, più la convergenza è lenta\n",
        "\n",
        "scegliere quanti livelli finetunare. vedere da pochi a molti livelli cosa succede, e sulla base di questo, fare esercizio 2, cercare di aumentare l'accuracy il più possibile sul test"
      ],
      "metadata": {
        "id": "kyOs4mx92koz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LAYER 4 + FC"
      ],
      "metadata": {
        "id": "k_XivuPVEhCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "lr = 0.01 # 0.01  #0.1\n",
        "epochs = 10\n",
        "device = torch.device(\"cuda\") # to use the GPU"
      ],
      "metadata": {
        "id": "eLRi1NacCa35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "\n",
        "# non faccio nessuna trasformazione a test time\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "\n",
        "trainset, valset = torch.utils.data.random_split(dataset, [40000, 10000]) # dataset, numero immagini train 40000 e validation 10000, in uscita avrò due dataset\n",
        "\n",
        "\n",
        "# costruisco i dataloader\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2,\n",
        "                                          drop_last=True)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2,\n",
        "                                          drop_last=False)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2,\n",
        "                                          drop_last=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxkpDelHCbuL",
        "outputId": "68ba3bf5-0d89-4f88-929c-a65045ddce81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = models.resnet18(pretrained=True)\n",
        "net.fc = nn.Linear(512, 10)\n",
        "net.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsNEls9KCm3i",
        "outputId": "7167ce8d-0f1d-4c5e-c8d7-ae1ad5457345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze layer4 parameters, all'interno 4° blocco della rete\n",
        "for param in net.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Unfreeze fc layer parameters\n",
        "net.fc.requires_grad = True\n",
        "\n",
        "print(count_trainable_parameters(net))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbPBTTTACynz",
        "outputId": "2cd1f6e1-e6a1-4d13-c322-2ed4e6a8a1a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8393728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer4_params = {'params': net.layer4.parameters(), 'lr': 0.001} # circa 7995000\n",
        "fc_params = {'params': net.fc.parameters(), 'lr': 0.1} # circa 5000\n",
        "# bisogna spostare poco quei layer che sono già stati addestrati, invce vanno spostati in maniera corretta i random\n",
        "\n",
        "\n",
        "# come si fa ad inizializzare un ottimizzatore pwe agire su questi parametri?\n",
        "# Assuming you are using an Adam optimizer\n",
        "optimizer = torch.optim.SGD([layer4_params, fc_params], momentum=0.9, weight_decay=1e-04) # 8393728\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "SolacsvfDABj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define train and test function\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 500 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "        losses.append(loss.item())\n",
        "    return np.mean(losses)\n",
        "\n",
        "def test(model, device, test_loader, val=False):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    mode = \"Val\" if val else \"Test\"\n",
        "    print('\\{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        mode,\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    test_acc = correct / len(test_loader.dataset)\n",
        "    return test_loss, test_acc"
      ],
      "metadata": {
        "id": "5yZSl_zFDBVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "model_state_dict = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train(net, device, trainloader, optimizer, epoch)\n",
        "    train_losses.append(train_loss)\n",
        "    val_loss, val_acc = test(net, device, valloader)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHrtShObDZFw",
        "outputId": "f91b253c-4fe3-4597-b8ec-b8d448d06697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.614350\n",
            "Train Epoch: 1 [32000/40000 (80%)]\tLoss: 1.438805\n",
            "\\Test set: Average loss: 205.7057, Accuracy: 5416/10000 (54%)\n",
            "\n",
            "Train Epoch: 2 [0/40000 (0%)]\tLoss: 1.541163\n",
            "Train Epoch: 2 [32000/40000 (80%)]\tLoss: 1.067788\n",
            "\\Test set: Average loss: 186.1315, Accuracy: 5801/10000 (58%)\n",
            "\n",
            "Train Epoch: 3 [0/40000 (0%)]\tLoss: 1.209922\n",
            "Train Epoch: 3 [32000/40000 (80%)]\tLoss: 1.176043\n",
            "\\Test set: Average loss: 178.8759, Accuracy: 5983/10000 (60%)\n",
            "\n",
            "Train Epoch: 4 [0/40000 (0%)]\tLoss: 1.067012\n",
            "Train Epoch: 4 [32000/40000 (80%)]\tLoss: 0.883793\n",
            "\\Test set: Average loss: 170.4402, Accuracy: 6162/10000 (62%)\n",
            "\n",
            "Train Epoch: 5 [0/40000 (0%)]\tLoss: 0.952084\n",
            "Train Epoch: 5 [32000/40000 (80%)]\tLoss: 1.138575\n",
            "\\Test set: Average loss: 167.1979, Accuracy: 6249/10000 (62%)\n",
            "\n",
            "Train Epoch: 6 [0/40000 (0%)]\tLoss: 1.121830\n",
            "Train Epoch: 6 [32000/40000 (80%)]\tLoss: 1.086853\n",
            "\\Test set: Average loss: 164.7648, Accuracy: 6331/10000 (63%)\n",
            "\n",
            "Train Epoch: 7 [0/40000 (0%)]\tLoss: 0.982751\n",
            "Train Epoch: 7 [32000/40000 (80%)]\tLoss: 1.018738\n",
            "\\Test set: Average loss: 162.6028, Accuracy: 6338/10000 (63%)\n",
            "\n",
            "Train Epoch: 8 [0/40000 (0%)]\tLoss: 0.836682\n",
            "Train Epoch: 8 [32000/40000 (80%)]\tLoss: 0.936713\n",
            "\\Test set: Average loss: 158.4451, Accuracy: 6463/10000 (65%)\n",
            "\n",
            "Train Epoch: 9 [0/40000 (0%)]\tLoss: 0.827997\n",
            "Train Epoch: 9 [32000/40000 (80%)]\tLoss: 1.122640\n",
            "\\Test set: Average loss: 153.9383, Accuracy: 6578/10000 (66%)\n",
            "\n",
            "Train Epoch: 10 [0/40000 (0%)]\tLoss: 0.948325\n",
            "Train Epoch: 10 [32000/40000 (80%)]\tLoss: 0.902487\n",
            "\\Test set: Average loss: 153.3922, Accuracy: 6577/10000 (66%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fare il load dei pesi sulla rete # QUI\n",
        "\n",
        "test_loss, test_acc = test(net, device, testloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLGZleRWDdvb",
        "outputId": "a6d1b6fa-d8f9-493a-bee8-4a189259b977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\Test set: Average loss: 154.3832, Accuracy: 6598/10000 (66%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LAYER 1 + FC"
      ],
      "metadata": {
        "id": "wmlGkdQPExA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "lr = 0.01 # 0.01  #0.1\n",
        "epochs = 10\n",
        "device = torch.device(\"cuda\") # to use the GPU"
      ],
      "metadata": {
        "id": "Rjd_ELq7E0hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "\n",
        "# non faccio nessuna trasformazione a test time\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "\n",
        "trainset, valset = torch.utils.data.random_split(dataset, [40000, 10000]) # dataset, numero immagini train 40000 e validation 10000, in uscita avrò due dataset\n",
        "\n",
        "\n",
        "# costruisco i dataloader\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2,\n",
        "                                          drop_last=True)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2,\n",
        "                                          drop_last=False)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2,\n",
        "                                          drop_last=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LM5m0yaFkOd",
        "outputId": "ff2d8e54-8e24-41d4-ed45-f2e195ee57ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = models.resnet18(pretrained=True)\n",
        "net.fc = nn.Linear(512, 10)\n",
        "net.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJcCIiN_FrnE",
        "outputId": "afacbef7-bf9b-4915-bb1c-8f1887823710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze layer1 parameters, all'interno 1° blocco della rete\n",
        "for param in net.layer1.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Unfreeze fc layer parameters\n",
        "net.fc.requires_grad = True\n",
        "\n",
        "print(count_trainable_parameters(net))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bsdDS2iFvTC",
        "outputId": "5e965018-caf9-49c2-b7af-904895f66103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "147968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer1_params = {'params': net.layer1.parameters(), 'lr': 0.001} # circa 7995000\n",
        "fc_params = {'params': net.fc.parameters(), 'lr': 0.1} # circa 5000\n",
        "# bisogna spostare poco quei layer che sono già stati addestrati, invce vanno spostati in maniera corretta i random\n",
        "\n",
        "\n",
        "# come si fa ad inizializzare un ottimizzatore pwe agire su questi parametri?\n",
        "# Assuming you are using an Adam optimizer\n",
        "optimizer = torch.optim.SGD([layer1_params, fc_params], momentum=0.9, weight_decay=1e-04) # 8393728\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "muox-r_KF4Gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define train and test function\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 500 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "        losses.append(loss.item())\n",
        "    return np.mean(losses)\n",
        "\n",
        "def test(model, device, test_loader, val=False):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    mode = \"Val\" if val else \"Test\"\n",
        "    print('\\{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        mode,\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    test_acc = correct / len(test_loader.dataset)\n",
        "    return test_loss, test_acc"
      ],
      "metadata": {
        "id": "qLy_63SXF4yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "model_state_dict = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train(net, device, trainloader, optimizer, epoch)\n",
        "    train_losses.append(train_loss)\n",
        "    val_loss, val_acc = test(net, device, valloader)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvMavlPRF97r",
        "outputId": "a8e67132-299a-45ed-e40b-f17730bfe0df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.564060\n",
            "Train Epoch: 1 [32000/40000 (80%)]\tLoss: 2.414378\n",
            "\\Test set: Average loss: 388.4275, Accuracy: 1279/10000 (13%)\n",
            "\n",
            "Train Epoch: 2 [0/40000 (0%)]\tLoss: 2.560171\n",
            "Train Epoch: 2 [32000/40000 (80%)]\tLoss: 2.358895\n",
            "\\Test set: Average loss: 370.7786, Accuracy: 1495/10000 (15%)\n",
            "\n",
            "Train Epoch: 3 [0/40000 (0%)]\tLoss: 2.266311\n",
            "Train Epoch: 3 [32000/40000 (80%)]\tLoss: 2.308924\n",
            "\\Test set: Average loss: 356.6931, Accuracy: 1735/10000 (17%)\n",
            "\n",
            "Train Epoch: 4 [0/40000 (0%)]\tLoss: 2.360715\n",
            "Train Epoch: 4 [32000/40000 (80%)]\tLoss: 2.143639\n",
            "\\Test set: Average loss: 349.4039, Accuracy: 1845/10000 (18%)\n",
            "\n",
            "Train Epoch: 5 [0/40000 (0%)]\tLoss: 2.267616\n",
            "Train Epoch: 5 [32000/40000 (80%)]\tLoss: 2.116818\n",
            "\\Test set: Average loss: 342.0324, Accuracy: 2064/10000 (21%)\n",
            "\n",
            "Train Epoch: 6 [0/40000 (0%)]\tLoss: 2.096284\n",
            "Train Epoch: 6 [32000/40000 (80%)]\tLoss: 2.246629\n",
            "\\Test set: Average loss: 329.6423, Accuracy: 2508/10000 (25%)\n",
            "\n",
            "Train Epoch: 7 [0/40000 (0%)]\tLoss: 2.007871\n",
            "Train Epoch: 7 [32000/40000 (80%)]\tLoss: 2.109543\n",
            "\\Test set: Average loss: 325.8242, Accuracy: 2652/10000 (27%)\n",
            "\n",
            "Train Epoch: 8 [0/40000 (0%)]\tLoss: 2.095774\n",
            "Train Epoch: 8 [32000/40000 (80%)]\tLoss: 2.011308\n",
            "\\Test set: Average loss: 318.9060, Accuracy: 2944/10000 (29%)\n",
            "\n",
            "Train Epoch: 9 [0/40000 (0%)]\tLoss: 2.061117\n",
            "Train Epoch: 9 [32000/40000 (80%)]\tLoss: 1.980489\n",
            "\\Test set: Average loss: 312.9111, Accuracy: 3107/10000 (31%)\n",
            "\n",
            "Train Epoch: 10 [0/40000 (0%)]\tLoss: 2.043378\n",
            "Train Epoch: 10 [32000/40000 (80%)]\tLoss: 1.872067\n",
            "\\Test set: Average loss: 308.9097, Accuracy: 3257/10000 (33%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fare il load dei pesi sulla rete # QUI\n",
        "\n",
        "test_loss, test_acc = test(net, device, testloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrBM820VGGLg",
        "outputId": "d10ee924-b644-4ca8-8714-7eb21bb74038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\Test set: Average loss: 320.9119, Accuracy: 2828/10000 (28%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LAYER 2 + FC"
      ],
      "metadata": {
        "id": "V2_77BToGW7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "lr = 0.01 # 0.01  #0.1\n",
        "epochs = 10\n",
        "device = torch.device(\"cuda\") # to use the GPU"
      ],
      "metadata": {
        "id": "GoLvr0pBG6xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "\n",
        "# non faccio nessuna trasformazione a test time\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "\n",
        "trainset, valset = torch.utils.data.random_split(dataset, [40000, 10000]) # dataset, numero immagini train 40000 e validation 10000, in uscita avrò due dataset\n",
        "\n",
        "\n",
        "# costruisco i dataloader\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2,\n",
        "                                          drop_last=True)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2,\n",
        "                                          drop_last=False)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2,\n",
        "                                          drop_last=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xapCbCxG6tE",
        "outputId": "db5c344c-6083-4a98-da0d-1f2d1eebe031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = models.resnet18(pretrained=True)\n",
        "net.fc = nn.Linear(512, 10)\n",
        "net.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUFoCQlFG6rj",
        "outputId": "9c879144-7a6c-4b3a-c333-aca9f2942434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze layer2 parameters, all'interno 2° blocco della rete\n",
        "for param in net.layer2.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Unfreeze fc layer parameters\n",
        "net.fc.requires_grad = True\n",
        "\n",
        "print(count_trainable_parameters(net))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bSyJjA-G6oa",
        "outputId": "49342104-dc41-4054-a1b2-dc5c6ef7e162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "525568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer2_params = {'params': net.layer2.parameters(), 'lr': 0.001} # circa 7995000\n",
        "fc_params = {'params': net.fc.parameters(), 'lr': 0.1} # circa 5000\n",
        "# bisogna spostare poco quei layer che sono già stati addestrati, invce vanno spostati in maniera corretta i random\n",
        "\n",
        "\n",
        "# come si fa ad inizializzare un ottimizzatore pwe agire su questi parametri?\n",
        "# Assuming you are using an Adam optimizer\n",
        "optimizer = torch.optim.SGD([layer2_params, fc_params], momentum=0.9, weight_decay=1e-04) # 8393728\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "q2jx7wRJG6m5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define train and test function\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 500 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "        losses.append(loss.item())\n",
        "    return np.mean(losses)\n",
        "\n",
        "def test(model, device, test_loader, val=False):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    mode = \"Val\" if val else \"Test\"\n",
        "    print('\\{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        mode,\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    test_acc = correct / len(test_loader.dataset)\n",
        "    return test_loss, test_acc"
      ],
      "metadata": {
        "id": "S3Rk8ralG6jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "model_state_dict = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train(net, device, trainloader, optimizer, epoch)\n",
        "    train_losses.append(train_loss)\n",
        "    val_loss, val_acc = test(net, device, valloader)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GMy6_OfG6iC",
        "outputId": "baedb9ed-140a-4127-b51d-d022ca128761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.666121\n",
            "Train Epoch: 1 [32000/40000 (80%)]\tLoss: 2.385042\n",
            "\\Test set: Average loss: 355.3454, Accuracy: 1914/10000 (19%)\n",
            "\n",
            "Train Epoch: 2 [0/40000 (0%)]\tLoss: 2.191776\n",
            "Train Epoch: 2 [32000/40000 (80%)]\tLoss: 2.129533\n",
            "\\Test set: Average loss: 316.3587, Accuracy: 2928/10000 (29%)\n",
            "\n",
            "Train Epoch: 3 [0/40000 (0%)]\tLoss: 2.015850\n",
            "Train Epoch: 3 [32000/40000 (80%)]\tLoss: 1.928674\n",
            "\\Test set: Average loss: 290.7067, Accuracy: 3692/10000 (37%)\n",
            "\n",
            "Train Epoch: 4 [0/40000 (0%)]\tLoss: 1.867131\n",
            "Train Epoch: 4 [32000/40000 (80%)]\tLoss: 1.668746\n",
            "\\Test set: Average loss: 264.4249, Accuracy: 4478/10000 (45%)\n",
            "\n",
            "Train Epoch: 5 [0/40000 (0%)]\tLoss: 1.777490\n",
            "Train Epoch: 5 [32000/40000 (80%)]\tLoss: 1.589396\n",
            "\\Test set: Average loss: 248.4393, Accuracy: 4935/10000 (49%)\n",
            "\n",
            "Train Epoch: 6 [0/40000 (0%)]\tLoss: 1.481752\n",
            "Train Epoch: 6 [32000/40000 (80%)]\tLoss: 1.529028\n",
            "\\Test set: Average loss: 230.0963, Accuracy: 5489/10000 (55%)\n",
            "\n",
            "Train Epoch: 7 [0/40000 (0%)]\tLoss: 1.420316\n",
            "Train Epoch: 7 [32000/40000 (80%)]\tLoss: 1.390484\n",
            "\\Test set: Average loss: 219.2320, Accuracy: 5780/10000 (58%)\n",
            "\n",
            "Train Epoch: 8 [0/40000 (0%)]\tLoss: 1.546083\n",
            "Train Epoch: 8 [32000/40000 (80%)]\tLoss: 1.241887\n",
            "\\Test set: Average loss: 207.8258, Accuracy: 6077/10000 (61%)\n",
            "\n",
            "Train Epoch: 9 [0/40000 (0%)]\tLoss: 1.255660\n",
            "Train Epoch: 9 [32000/40000 (80%)]\tLoss: 1.226999\n",
            "\\Test set: Average loss: 196.4760, Accuracy: 6304/10000 (63%)\n",
            "\n",
            "Train Epoch: 10 [0/40000 (0%)]\tLoss: 1.376005\n",
            "Train Epoch: 10 [32000/40000 (80%)]\tLoss: 1.399691\n",
            "\\Test set: Average loss: 187.4399, Accuracy: 6470/10000 (65%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fare il load dei pesi sulla rete # QUI\n",
        "\n",
        "test_loss, test_acc = test(net, device, testloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaEnOppaG6e1",
        "outputId": "4478a0a6-e7fd-4279-ffd8-873ad6175e74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\Test set: Average loss: 179.1748, Accuracy: 6697/10000 (67%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LAYER 3 + FC\n"
      ],
      "metadata": {
        "id": "ieMsfXeyIZAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "lr = 0.01 # 0.01  #0.1\n",
        "epochs = 10\n",
        "device = torch.device(\"cuda\") # to use the GPU"
      ],
      "metadata": {
        "id": "myfIYWPgIb2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "\n",
        "# non faccio nessuna trasformazione a test time\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "\n",
        "trainset, valset = torch.utils.data.random_split(dataset, [40000, 10000]) # dataset, numero immagini train 40000 e validation 10000, in uscita avrò due dataset\n",
        "\n",
        "\n",
        "# costruisco i dataloader\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2,\n",
        "                                          drop_last=True)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2,\n",
        "                                          drop_last=False)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2,\n",
        "                                          drop_last=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cs6I1oHIby1",
        "outputId": "7adccceb-614f-4068-e6c9-9fd7302cbacd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = models.resnet18(pretrained=True)\n",
        "net.fc = nn.Linear(512, 10)\n",
        "net.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SF47RW6FIbxV",
        "outputId": "d6b4cd1e-d8e9-49b6-d867-d84d34e0f095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze layer3 parameters, all'interno 3° blocco della rete\n",
        "for param in net.layer3.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Unfreeze fc layer parameters\n",
        "net.fc.requires_grad = True\n",
        "\n",
        "print(count_trainable_parameters(net))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H3MTj0dG6dM",
        "outputId": "08ad4115-2971-4bed-e187-cd2811d41f02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2099712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer3_params = {'params': net.layer3.parameters(), 'lr': 0.001} # circa 7995000\n",
        "fc_params = {'params': net.fc.parameters(), 'lr': 0.1} # circa 5000\n",
        "# bisogna spostare poco quei layer che sono già stati addestrati, invce vanno spostati in maniera corretta i random\n",
        "\n",
        "\n",
        "# come si fa ad inizializzare un ottimizzatore pwe agire su questi parametri?\n",
        "# Assuming you are using an Adam optimizer\n",
        "optimizer = torch.optim.SGD([layer3_params, fc_params], momentum=0.9, weight_decay=1e-04) # 8393728\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "igubCg0sG6ZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define train and test function\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 500 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "        losses.append(loss.item())\n",
        "    return np.mean(losses)\n",
        "\n",
        "def test(model, device, test_loader, val=False):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    mode = \"Val\" if val else \"Test\"\n",
        "    print('\\{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        mode,\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    test_acc = correct / len(test_loader.dataset)\n",
        "    return test_loss, test_acc"
      ],
      "metadata": {
        "id": "FRyqT189G6Xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "model_state_dict = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train(net, device, trainloader, optimizer, epoch)\n",
        "    train_losses.append(train_loss)\n",
        "    val_loss, val_acc = test(net, device, valloader)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkYZWV1FG6T4",
        "outputId": "ab204a06-a3cc-4005-e999-d1dfbe9be9bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.658584\n",
            "Train Epoch: 1 [32000/40000 (80%)]\tLoss: 2.011906\n",
            "\\Test set: Average loss: 312.6906, Accuracy: 3004/10000 (30%)\n",
            "\n",
            "Train Epoch: 2 [0/40000 (0%)]\tLoss: 2.132085\n",
            "Train Epoch: 2 [32000/40000 (80%)]\tLoss: 1.796978\n",
            "\\Test set: Average loss: 253.4689, Accuracy: 4860/10000 (49%)\n",
            "\n",
            "Train Epoch: 3 [0/40000 (0%)]\tLoss: 1.617855\n",
            "Train Epoch: 3 [32000/40000 (80%)]\tLoss: 1.316226\n",
            "\\Test set: Average loss: 216.1707, Accuracy: 5799/10000 (58%)\n",
            "\n",
            "Train Epoch: 4 [0/40000 (0%)]\tLoss: 1.577679\n",
            "Train Epoch: 4 [32000/40000 (80%)]\tLoss: 1.441144\n",
            "\\Test set: Average loss: 192.5191, Accuracy: 6317/10000 (63%)\n",
            "\n",
            "Train Epoch: 5 [0/40000 (0%)]\tLoss: 1.253528\n",
            "Train Epoch: 5 [32000/40000 (80%)]\tLoss: 1.241703\n",
            "\\Test set: Average loss: 175.9707, Accuracy: 6622/10000 (66%)\n",
            "\n",
            "Train Epoch: 6 [0/40000 (0%)]\tLoss: 1.003162\n",
            "Train Epoch: 6 [32000/40000 (80%)]\tLoss: 1.272495\n",
            "\\Test set: Average loss: 166.5666, Accuracy: 6809/10000 (68%)\n",
            "\n",
            "Train Epoch: 7 [0/40000 (0%)]\tLoss: 1.065634\n",
            "Train Epoch: 7 [32000/40000 (80%)]\tLoss: 1.345860\n",
            "\\Test set: Average loss: 157.9194, Accuracy: 6917/10000 (69%)\n",
            "\n",
            "Train Epoch: 8 [0/40000 (0%)]\tLoss: 0.806602\n",
            "Train Epoch: 8 [32000/40000 (80%)]\tLoss: 1.068419\n",
            "\\Test set: Average loss: 149.5791, Accuracy: 7064/10000 (71%)\n",
            "\n",
            "Train Epoch: 9 [0/40000 (0%)]\tLoss: 0.971406\n",
            "Train Epoch: 9 [32000/40000 (80%)]\tLoss: 1.100595\n",
            "\\Test set: Average loss: 148.3107, Accuracy: 7092/10000 (71%)\n",
            "\n",
            "Train Epoch: 10 [0/40000 (0%)]\tLoss: 1.040161\n",
            "Train Epoch: 10 [32000/40000 (80%)]\tLoss: 1.102096\n",
            "\\Test set: Average loss: 143.1942, Accuracy: 7144/10000 (71%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fare il load dei pesi sulla rete # QUI\n",
        "\n",
        "test_loss, test_acc = test(net, device, testloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGBFYuJUG6SX",
        "outputId": "43cb3d53-a402-4822-bea4-7f7f4d28b904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\Test set: Average loss: 136.2675, Accuracy: 7373/10000 (74%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2\n",
        "\n",
        "Try to change the hyper-parameters of the fine-tuning (e.g. lr of CNN layers and lr of the fc layers) and/or network architecture"
      ],
      "metadata": {
        "id": "cUunhER-2y9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LAYER 1-4 + FC con lr = 0.001"
      ],
      "metadata": {
        "id": "_BhVJQDCK-LV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "lr = 0.01 # 0.01  #0.1\n",
        "epochs = 10\n",
        "device = torch.device(\"cuda\") # to use the GPU\n"
      ],
      "metadata": {
        "id": "zjVDlk_YLFHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "\n",
        "# non faccio nessuna trasformazione a test time\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "\n",
        "trainset, valset = torch.utils.data.random_split(dataset, [40000, 10000]) # dataset, numero immagini train 40000 e validation 10000, in uscita avrò due dataset\n",
        "\n",
        "\n",
        "# costruisco i dataloader\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2,\n",
        "                                          drop_last=True)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2,\n",
        "                                          drop_last=False)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2,\n",
        "                                          drop_last=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJ1JYLXnLFFk",
        "outputId": "561cba88-fbf8-4b8e-fc3d-8d14158ce11c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = models.resnet18(pretrained=True)\n",
        "net.fc = nn.Linear(512, 10)\n",
        "net.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDtxU3fuLFCY",
        "outputId": "e187444c-65f0-49ee-9e50-eca5f20478c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_trainable_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad) # requires_grad mi dice se è freezato o learnable, TRUE deve essere appreso, FALSE no, ritorna la somma dei parametri che richiedono gradiente\n",
        "count_trainable_parameters(net)\n",
        "\n",
        "for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "#Unfreeze layer1 parameters, all'interno 1° blocco della rete\n",
        "for param in net.layer1.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in net.layer2.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in net.layer3.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in net.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Unfreeze fc layer parameters\n",
        "net.fc.requires_grad = True\n",
        "\n",
        "#net.fc.weight.requires_grad = True\n",
        "#net.fc.bias.requires_grad = True\n",
        "\n",
        "print(count_trainable_parameters(net))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emhWfr1hLFA1",
        "outputId": "6a3f3edf-fd11-4364-dbc0-ea9d241d3db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11166976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer1_params = {'params': net.layer1.parameters(), 'lr': 0.001}\n",
        "layer2_params = {'params': net.layer2.parameters(), 'lr': 0.001}\n",
        "layer3_params = {'params': net.layer3.parameters(), 'lr': 0.001}\n",
        "layer4_params = {'params': net.layer4.parameters(), 'lr': 0.001}\n",
        "fc_params = {'params': net.fc.parameters(), 'lr': 0.1} # circa 5000\n",
        "# bisogna spostare poco quei layer che sono già stati addestrati, invce vanno spostati in maniera corretta i random\n",
        "\n",
        "\n",
        "# come si fa ad inizializzare un ottimizzatore pwe agire su questi parametri?\n",
        "# Assuming you are using an Adam optimizer\n",
        "optimizer = torch.optim.SGD([layer1_params, layer2_params, layer3_params, layer4_params, fc_params], momentum=0.9, weight_decay=1e-04) # 8393728\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "j5pz4MWRLE93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define train and test function\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 500 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "        losses.append(loss.item())\n",
        "    return np.mean(losses)\n",
        "\n",
        "def test(model, device, test_loader, val=False):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    mode = \"Val\" if val else \"Test\"\n",
        "    print('\\{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        mode,\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    test_acc = correct / len(test_loader.dataset)\n",
        "    return test_loss, test_acc"
      ],
      "metadata": {
        "id": "5nuOTJOfLE8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "model_state_dict = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train(net, device, trainloader, optimizer, epoch)\n",
        "    train_losses.append(train_loss)\n",
        "    val_loss, val_acc = test(net, device, valloader)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKqQzzWiLE5f",
        "outputId": "a0182ebb-ec14-4f92-e561-ef3c01f6730f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.617908\n",
            "Train Epoch: 1 [32000/40000 (80%)]\tLoss: 1.077149\n",
            "\\Test set: Average loss: 144.7487, Accuracy: 6812/10000 (68%)\n",
            "\n",
            "Train Epoch: 2 [0/40000 (0%)]\tLoss: 0.738457\n",
            "Train Epoch: 2 [32000/40000 (80%)]\tLoss: 1.102464\n",
            "\\Test set: Average loss: 122.3912, Accuracy: 7349/10000 (73%)\n",
            "\n",
            "Train Epoch: 3 [0/40000 (0%)]\tLoss: 0.673748\n",
            "Train Epoch: 3 [32000/40000 (80%)]\tLoss: 0.850887\n",
            "\\Test set: Average loss: 110.5737, Accuracy: 7529/10000 (75%)\n",
            "\n",
            "Train Epoch: 4 [0/40000 (0%)]\tLoss: 0.760127\n",
            "Train Epoch: 4 [32000/40000 (80%)]\tLoss: 0.629086\n",
            "\\Test set: Average loss: 104.7687, Accuracy: 7672/10000 (77%)\n",
            "\n",
            "Train Epoch: 5 [0/40000 (0%)]\tLoss: 0.662762\n",
            "Train Epoch: 5 [32000/40000 (80%)]\tLoss: 0.574579\n",
            "\\Test set: Average loss: 102.8108, Accuracy: 7719/10000 (77%)\n",
            "\n",
            "Train Epoch: 6 [0/40000 (0%)]\tLoss: 0.661864\n",
            "Train Epoch: 6 [32000/40000 (80%)]\tLoss: 0.678284\n",
            "\\Test set: Average loss: 96.7023, Accuracy: 7836/10000 (78%)\n",
            "\n",
            "Train Epoch: 7 [0/40000 (0%)]\tLoss: 0.852176\n",
            "Train Epoch: 7 [32000/40000 (80%)]\tLoss: 0.552949\n",
            "\\Test set: Average loss: 96.1845, Accuracy: 7895/10000 (79%)\n",
            "\n",
            "Train Epoch: 8 [0/40000 (0%)]\tLoss: 0.566027\n",
            "Train Epoch: 8 [32000/40000 (80%)]\tLoss: 0.387942\n",
            "\\Test set: Average loss: 91.7648, Accuracy: 7970/10000 (80%)\n",
            "\n",
            "Train Epoch: 9 [0/40000 (0%)]\tLoss: 0.461777\n",
            "Train Epoch: 9 [32000/40000 (80%)]\tLoss: 0.343445\n",
            "\\Test set: Average loss: 90.9188, Accuracy: 7980/10000 (80%)\n",
            "\n",
            "Train Epoch: 10 [0/40000 (0%)]\tLoss: 0.470242\n",
            "Train Epoch: 10 [32000/40000 (80%)]\tLoss: 0.408752\n",
            "\\Test set: Average loss: 88.5543, Accuracy: 8091/10000 (81%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fare il load dei pesi sulla rete # QUI\n",
        "\n",
        "test_loss, test_acc = test(net, device, testloader)"
      ],
      "metadata": {
        "id": "8xQVd_AQLE4B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48bfff61-46d7-4438-81bb-ab29bc01c3b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\Test set: Average loss: 84.0407, Accuracy: 8188/10000 (82%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LAYER 1-4 + FC con lr = 0.0001 (1-4)"
      ],
      "metadata": {
        "id": "onzTHpXBQz8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "lr = 0.01 # 0.01  #0.1\n",
        "epochs = 10\n",
        "device = torch.device(\"cpu\") # to use the GPU"
      ],
      "metadata": {
        "id": "W3bU2WZNLE1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "\n",
        "# non faccio nessuna trasformazione a test time\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "\n",
        "trainset, valset = torch.utils.data.random_split(dataset, [40000, 10000]) # dataset, numero immagini train 40000 e validation 10000, in uscita avrò due dataset\n",
        "\n",
        "\n",
        "# costruisco i dataloader\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2,\n",
        "                                          drop_last=True)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2,\n",
        "                                          drop_last=False)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2,\n",
        "                                          drop_last=False)"
      ],
      "metadata": {
        "id": "Sx2jr8XVLEzz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50eed819-5e95-478b-ffa2-ba3b5b26c7ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = models.resnet18(pretrained=True)\n",
        "net.fc = nn.Linear(512, 10)\n",
        "net.to(device)"
      ],
      "metadata": {
        "id": "CsqovR_gLEtV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b6cac0e-e99f-4345-96e3-2c5690c69fd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_trainable_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad) # requires_grad mi dice se è freezato o learnable, TRUE deve essere appreso, FALSE no, ritorna la somma dei parametri che richiedono gradiente\n",
        "count_trainable_parameters(net)\n",
        "\n",
        "for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "#Unfreeze layer1 parameters, all'interno 1° blocco della rete\n",
        "for param in net.layer1.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in net.layer2.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in net.layer3.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in net.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Unfreeze fc layer parameters\n",
        "net.fc.requires_grad = True\n",
        "\n",
        "#net.fc.weight.requires_grad = True\n",
        "#net.fc.bias.requires_grad = True\n",
        "\n",
        "print(count_trainable_parameters(net))"
      ],
      "metadata": {
        "id": "S5xmQtlBRSVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3347513b-73c0-4bc3-cd47-fa5e0e3b39fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11166976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer1_params = {'params': net.layer1.parameters(), 'lr': 0.0001}\n",
        "layer2_params = {'params': net.layer2.parameters(), 'lr': 0.0001}\n",
        "layer3_params = {'params': net.layer3.parameters(), 'lr': 0.0001}\n",
        "layer4_params = {'params': net.layer4.parameters(), 'lr': 0.0001}\n",
        "fc_params = {'params': net.fc.parameters(), 'lr': 0.1} # circa 5000\n",
        "# bisogna spostare poco quei layer che sono già stati addestrati, invce vanno spostati in maniera corretta i random\n",
        "\n",
        "\n",
        "# come si fa ad inizializzare un ottimizzatore pwe agire su questi parametri?\n",
        "# Assuming you are using an Adam optimizer\n",
        "optimizer = torch.optim.SGD([layer1_params, layer2_params, layer3_params, layer4_params, fc_params], momentum=0.9, weight_decay=1e-04) # 8393728\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "1ZZgCbJpRSMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define train and test function\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 500 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "        losses.append(loss.item())\n",
        "    return np.mean(losses)\n",
        "\n",
        "def test(model, device, test_loader, val=False):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    mode = \"Val\" if val else \"Test\"\n",
        "    print('\\{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        mode,\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    test_acc = correct / len(test_loader.dataset)\n",
        "    return test_loss, test_acc"
      ],
      "metadata": {
        "id": "N0Gaso9QRSEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "model_state_dict = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train(net, device, trainloader, optimizer, epoch)\n",
        "    train_losses.append(train_loss)\n",
        "    val_loss, val_acc = test(net, device, valloader)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)"
      ],
      "metadata": {
        "id": "_nEbHagtRSC0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "71ee7d76-8801-4671-f329-c307c3480d7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.580923\n",
            "Train Epoch: 1 [32000/40000 (80%)]\tLoss: 1.802090\n",
            "\\Test set: Average loss: 244.1810, Accuracy: 4660/10000 (47%)\n",
            "\n",
            "Train Epoch: 2 [0/40000 (0%)]\tLoss: 1.232132\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-8d0d2cf66791>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-af648af55195>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fare il load dei pesi sulla rete # QUI\n",
        "\n",
        "test_loss, test_acc = test(net, device, testloader)"
      ],
      "metadata": {
        "id": "chkGkwiWRSAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3\n",
        "\n",
        "Try to implement the model selection strategy (also known as early stopping) based on the validation accuracy on cifar10.\n",
        "\n",
        "Consider using the two following command to respectively save and load the state of all the parameters of the model in a moment.\n",
        "\n",
        "vorrei cercare il modello con più alta performance, prendere stato parametri e salvarmela da un'altra parte\n",
        "\n",
        "implementare una strategia di model selection (quello che si diceva nell'early stopping): all'interno delle mie 10 epoche, io vorrei andare a cercare qual è il modello che ha la più alta performance, all'interno di queste epoche, e prendere lo stato dei suoi parametri e salvarmelo da una parte. Poi questa parte io la prendo e, alla fine del mio training, nel mio setting migliore, lo voglio ri-inizializzare sul modello e usare quello a test time."
      ],
      "metadata": {
        "id": "GuygmHB43UHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save all the parameters of the model\n",
        "model_state_dict = net.state_dict()\n",
        "# ritorna il dizionario dei parametri. lo saliamo quando abbiamo la migliore accuratezza sul validation set\n",
        "# in model_state_dict avrò il dizionario con tutti i layer della rete con tutti i valori dei parametri associati a quei layer\n",
        "# questo dizionario è la rete, la rete la fanno i pesi\n",
        "# si salva quando ho la migliore accuratezza sul validation set\n",
        "\n",
        "\n",
        "# nel momento in cui è finito il training, ad es 100 epoche, se la best accuracy l'ho trovata alla epoca 60, ricarichiamo i pesi dell'epoca 60 nel modello per poi usarla a test time\n",
        "\n",
        "\n",
        "# load saved weights on the model\n",
        "net.load_state_dict(model_state_dict)\n",
        "# ricarico il modello, carica un dizionario di parametri del modello, sovrascrive tutti parametri della rete, gli passiamo lo state_dict dell'epoca migliore\n",
        "\n",
        "\n",
        "# modificare il loop di training, prima di chiamare la funzione che fa il test (#QUI) facciamo il load dello state dict sulla rete per ricaricare i pesi migliori"
      ],
      "metadata": {
        "id": "uOhllTzr32Ld",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e23e2dc-0e2a-480b-d848-64081046a514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAVE MODEL & LOAD MODEL"
      ],
      "metadata": {
        "id": "v0KqGX6WFeXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "lr = 0.01 # 0.01  #0.1\n",
        "epochs = 20\n",
        "device = torch.device(\"cuda\") # to use the GPU\n"
      ],
      "metadata": {
        "id": "0p4El5Dg3Ag3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "\n",
        "# non faccio nessuna trasformazione a test time\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "\n",
        "trainset, valset = torch.utils.data.random_split(dataset, [40000, 10000]) # dataset, numero immagini train 40000 e validation 10000, in uscita avrò due dataset\n",
        "\n",
        "\n",
        "# costruisco i dataloader\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2,\n",
        "                                          drop_last=True)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2,\n",
        "                                          drop_last=False)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2,\n",
        "                                          drop_last=False)"
      ],
      "metadata": {
        "id": "mHxIYumU4Vov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81e9fa7c-fd3b-4a7e-f1a3-6063e832c548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = models.resnet18(pretrained=True)\n",
        "net.fc = nn.Linear(512, 10)\n",
        "net.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooKJYIDzFzHS",
        "outputId": "6142eaae-c5a1-4a7d-89ea-865153664d65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_trainable_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad) # requires_grad mi dice se è freezato o learnable, TRUE deve essere appreso, FALSE no, ritorna la somma dei parametri che richiedono gradiente\n",
        "count_trainable_parameters(net)\n",
        "\n",
        "for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "#Unfreeze layer1 parameters, all'interno 1° blocco della rete\n",
        "for param in net.layer1.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in net.layer2.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in net.layer3.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in net.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Unfreeze fc layer parameters\n",
        "net.fc.requires_grad = True\n",
        "\n",
        "#net.fc.weight.requires_grad = True\n",
        "#net.fc.bias.requires_grad = True\n",
        "\n",
        "print(count_trainable_parameters(net))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lG1TySTfFzFa",
        "outputId": "a3fdeb01-7f01-4929-b415-2543345a0c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11166976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer1_params = {'params': net.layer1.parameters(), 'lr': 0.001}\n",
        "layer2_params = {'params': net.layer2.parameters(), 'lr': 0.001}\n",
        "layer3_params = {'params': net.layer3.parameters(), 'lr': 0.001}\n",
        "layer4_params = {'params': net.layer4.parameters(), 'lr': 0.001}\n",
        "fc_params = {'params': net.fc.parameters(), 'lr': 0.1} # circa 5000\n",
        "# bisogna spostare poco quei layer che sono già stati addestrati, invce vanno spostati in maniera corretta i random\n",
        "\n",
        "\n",
        "# come si fa ad inizializzare un ottimizzatore pwe agire su questi parametri?\n",
        "# Assuming you are using an Adam optimizer\n",
        "optimizer = torch.optim.SGD([layer1_params, layer2_params, layer3_params, layer4_params, fc_params], momentum=0.9, weight_decay=1e-04) # 8393728\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "MnCCH5fpFy8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define train and test function\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 500 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "        losses.append(loss.item())\n",
        "    return np.mean(losses)\n",
        "\n",
        "def test(model, device, test_loader, val=False):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    mode = \"Val\" if val else \"Test\"\n",
        "    print('\\{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        mode,\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    test_acc = correct / len(test_loader.dataset)\n",
        "    return test_loss, test_acc"
      ],
      "metadata": {
        "id": "6wGL488rFy3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "current_val_loss = np.inf\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train(net, device, trainloader, optimizer, epoch)\n",
        "    train_losses.append(train_loss)\n",
        "    val_loss, val_acc = test(net, device, valloader)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "    if val_loss <= current_val_loss:\n",
        "      current_val_loss = val_loss\n",
        "      torch.save(net.state_dict(), f'model_parameters.pth')\n",
        "    else:\n",
        "      break\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9Y-cD_oFy1F",
        "outputId": "e9a1270b-2456-4d51-f6ae-0310ff3d8189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.627929\n",
            "Train Epoch: 1 [32000/40000 (80%)]\tLoss: 0.918371\n",
            "\\Test set: Average loss: 144.5766, Accuracy: 6804/10000 (68%)\n",
            "\n",
            "Train Epoch: 2 [0/40000 (0%)]\tLoss: 0.900742\n",
            "Train Epoch: 2 [32000/40000 (80%)]\tLoss: 0.562562\n",
            "\\Test set: Average loss: 123.2627, Accuracy: 7267/10000 (73%)\n",
            "\n",
            "Train Epoch: 3 [0/40000 (0%)]\tLoss: 0.806990\n",
            "Train Epoch: 3 [32000/40000 (80%)]\tLoss: 0.679145\n",
            "\\Test set: Average loss: 111.6143, Accuracy: 7541/10000 (75%)\n",
            "\n",
            "Train Epoch: 4 [0/40000 (0%)]\tLoss: 0.490635\n",
            "Train Epoch: 4 [32000/40000 (80%)]\tLoss: 0.603256\n",
            "\\Test set: Average loss: 104.6539, Accuracy: 7675/10000 (77%)\n",
            "\n",
            "Train Epoch: 5 [0/40000 (0%)]\tLoss: 0.647885\n",
            "Train Epoch: 5 [32000/40000 (80%)]\tLoss: 0.811978\n",
            "\\Test set: Average loss: 100.0111, Accuracy: 7824/10000 (78%)\n",
            "\n",
            "Train Epoch: 6 [0/40000 (0%)]\tLoss: 0.966464\n",
            "Train Epoch: 6 [32000/40000 (80%)]\tLoss: 0.655296\n",
            "\\Test set: Average loss: 96.5366, Accuracy: 7904/10000 (79%)\n",
            "\n",
            "Train Epoch: 7 [0/40000 (0%)]\tLoss: 0.758326\n",
            "Train Epoch: 7 [32000/40000 (80%)]\tLoss: 0.618756\n",
            "\\Test set: Average loss: 91.9375, Accuracy: 8001/10000 (80%)\n",
            "\n",
            "Train Epoch: 8 [0/40000 (0%)]\tLoss: 0.537250\n",
            "Train Epoch: 8 [32000/40000 (80%)]\tLoss: 0.436006\n",
            "\\Test set: Average loss: 89.5385, Accuracy: 8059/10000 (81%)\n",
            "\n",
            "Train Epoch: 9 [0/40000 (0%)]\tLoss: 0.405363\n",
            "Train Epoch: 9 [32000/40000 (80%)]\tLoss: 0.638835\n",
            "\\Test set: Average loss: 88.4702, Accuracy: 8066/10000 (81%)\n",
            "\n",
            "Train Epoch: 10 [0/40000 (0%)]\tLoss: 0.415261\n",
            "Train Epoch: 10 [32000/40000 (80%)]\tLoss: 0.451892\n",
            "\\Test set: Average loss: 87.7398, Accuracy: 8094/10000 (81%)\n",
            "\n",
            "Train Epoch: 11 [0/40000 (0%)]\tLoss: 0.417619\n",
            "Train Epoch: 11 [32000/40000 (80%)]\tLoss: 0.450666\n",
            "\\Test set: Average loss: 88.4341, Accuracy: 8114/10000 (81%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load saved weights on the model\n",
        "net.load_state_dict(torch.load(f'model_parameters.pth'))\n",
        "\n",
        "test_loss, test_acc = test(net, device, testloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31PVfCnbGOxh",
        "outputId": "dca94e0c-db67-4476-bb40-074f2b39b86b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\Test set: Average loss: 83.3539, Accuracy: 8219/10000 (82%)\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}