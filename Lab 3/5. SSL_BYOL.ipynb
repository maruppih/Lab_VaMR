{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIhoHTwntu1V"
      },
      "source": [
        "# Bootstrap Your Own Latent (BYOL)\n",
        "\n",
        "In this session we are going to implement Bootstrap Your Own Latent paper (https://arxiv.org/abs/2006.07733).\n",
        "\n",
        "It uses a MoCo-style training (with asymmetric SiameseNet) but with a L2 loss penalty (it is not a contrastive-base method)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L'implementazione BYOL è un po' più complessa degli altri metodi. \n",
        "\n",
        "BYOL si basa su MoCo ma quest'ultimo usa la memoria per salvarsi le precedenti positive per poi usarle come negative. I BYOL la memoria non serve più, perché non è basato su Contrastive Loss, quindi non abbiamo più proprio il concetto di positive e negative, quindi non serve nemmeno più la memoria. Usa una distanza L2\n",
        "\n",
        "BYOL però da MoCo prende la struttura asimmetrica della siamese network. Ha una target network che è la copia dello student network, ma che viene aggiornata in modo particolare. Questo aggiornamento della target network è chiamato EMA update.\n",
        "\n",
        "BYOL deve gestire questa asimmetria delle reti e poi anche questo diverso aggiornamento dei pesi: lo student viene aggiornato con SGD normalmente (o con un ottimizzatore normale), mentre la teacher viene aggiornato con una tecnica di momentum.\n",
        "\n",
        "barlow twins vs byol: memoria, byol non è basato su contrastive loss, non serve neppure più la memoria, utilizza una loss l2 (distanza euclidea tra features, tra due versioni augmented della stessa immagine). riprende la struttura asimmetrica della siamese network. l'aggiornamento della target net si chiama ema.\n",
        "\n",
        "qui non servirà la siamese network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVw6H3IEJVuk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision.io import read_image\n",
        "\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from PIL import Image\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0Yv5FeDvVm8"
      },
      "outputs": [],
      "source": [
        "# loss fn\n",
        "def loss_fn(x, y): # normalizza le features\n",
        "    x = F.normalize(x, dim=-1, p=2) # p è l'esponente\n",
        "    y = F.normalize(y, dim=-1, p=2)\n",
        "    return 2 - 2 * (x * y).sum(dim=-1) # quadrato di un binomio, meno 2 volte il prodotto tra loro due\n",
        "\n",
        "\n",
        "class EMA():\n",
        "    # exponential moving average\n",
        "    def __init__(self, beta):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "\n",
        "    def update_average(self, old, new): # i parametri vecchi e parametri nuovi\n",
        "        if old is None:\n",
        "            return new\n",
        "        return old * self.beta + (1 - self.beta) * new # classico momentum in codice\n",
        "\n",
        "def update_moving_average(ema_updater, ma_model, current_model):\n",
        "    for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()): # zip prende un parametro da una lista e poi dall'altra, vuole due iteratori\n",
        "        old_weight, up_weight = ma_params.data, current_params.data # si aggiorna la rete così per ogni parametro della rete\n",
        "        ma_params.data = ema_updater.update_average(old_weight, up_weight)\n",
        "\n",
        "# MLP class for projector and predictor\n",
        "\n",
        "def MLP(dim, projection_size, hidden_size=4096, sync_batchnorm=None):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(dim, hidden_size),\n",
        "        nn.BatchNorm1d(hidden_size),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(hidden_size, projection_size)\n",
        "    )\n",
        "\n",
        "\n",
        "class BYOL(nn.Module): # non è più una loss, ma bensì ha dentro anche la parte di rete. L'uscità non saranno più le features, ma direttamente la loss\n",
        "    def __init__(self, backbone, moving_average_decay = 0.99): # prende la backbone (siamese) e basta, il mad è il lambda con valore di default\n",
        "        super().__init__()\n",
        "\n",
        "        self.target_ema_updater = EMA(moving_average_decay)\n",
        "\n",
        "        self.online_net = backbone # la backbone viene usata per definire la online net\n",
        "        self.online_net.fc = nn.Identity() # togliamo fc e mettiamo la identity, il fc non viene sovrascritto (immagine rete) il project ce lo h solo la online network\n",
        "        self.online_projector = MLP(512, 512, 4096) # il project è un mlp, solo l'online network ha un projector aggiuntivo\n",
        "\n",
        "        self.target_net = None # qui non lo consideriamo il projector\n",
        "\n",
        "    def _get_target_encoder(self): # prende la target net come copia della online net\n",
        "        if self.target_net is None:\n",
        "          # init target net\n",
        "          target_net = copy.deepcopy(self.online_net)\n",
        "          for p in target_net.parameters():\n",
        "              p.requires_grad = False\n",
        "          self.target_net = target_net\n",
        "        else:\n",
        "          target_net = self.target_net\n",
        "        return target_net\n",
        "\n",
        "    def update_moving_average(self):\n",
        "        update_moving_average(self.target_ema_updater, self.target_net, self.online_net) # prende la rete allo stato attuale..\n",
        "\n",
        "    def forward(self, x1, x2): # qui non arrivano le features, ma direttamente le immagini\n",
        "\n",
        "        images = torch.cat((x1, x2), dim = 0) # concatenazione versione augmented\n",
        "\n",
        "        online_projections = self.online_projector(self.online_net(images)) # q_theta di zeta_theta, prendo le features delle encoder e poi ci si appende un predictor in più\n",
        "        online_pred_one, online_pred_two = online_projections.chunk(2, dim = 0) # ri-splitto le projection sulle righe, chunck divide un vettore in toto (2) dimensioni\n",
        "\n",
        "        with torch.no_grad(): # di tutto quello che c'è qui dentro a noi non interessa calcolarci i gradienti, non vogliamo farci la backprop\n",
        "            target_net = self._get_target_encoder() # si prende il target encoder # ogni volta copia la online net, ma non va bene ( _get_target_encoder!! (corretta)\n",
        "\n",
        "            target_projections = target_net(images) # si fa forward della rete\n",
        "            target_projections = target_projections.detach()\n",
        "            target_proj_one, target_proj_two = target_projections.chunk(2, dim = 0) # rifacciamo i chunk\n",
        "\n",
        "            # perché (immagine) si fa il forward delle due versioni augmented? la versione online ne processa una (x1), la target un'altra (x2), ma perché è comodo farlo con entrambe le reti?\n",
        "            # (anche o vede x2 e t x2)\n",
        "            # perché così è più efficiente! altrimenti sarebbe l1 (vedi disegno)\n",
        "            # ecco perché vengono calcolate due loss e poi sommate, di cui poi si prende la media\n",
        "\n",
        "        loss_one = loss_fn(online_pred_one, target_proj_two.detach())\n",
        "        loss_two = loss_fn(online_pred_two, target_proj_one.detach())\n",
        "\n",
        "        loss = loss_one + loss_two\n",
        "        return loss.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTq2njrMrvkM"
      },
      "source": [
        "## Exercise 0\n",
        "\n",
        "Study the above code.\n",
        "- Where is the EMA updates?\n",
        "- Why it computes both loss_one and loss_two values?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3lt3jwGryO-"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "Write the training loop for moco-style training as used in BYOL.\n",
        "Use the Dataset which creates the two augmented views for each image and the Siamese Network from the past lab session [1](https://colab.research.google.com/drive/1NJwAFbRiD4MdwWf__6P2Lm0xYk_DNdVu?usp=sharing) and [2](https://colab.research.google.com/drive/1AMkh0q8L5nJScx7v6cMWoK336zqOqDY6?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQHjatntCQEU"
      },
      "outputs": [],
      "source": [
        "backbone = models.resnet18()\n",
        "#print(backbone)\n",
        "\n",
        "#! remember to delete the fc layer (we need just the CNN layers + flatten)\n",
        "backbone.fc = nn.Identity()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwZIsBcYCYFE"
      },
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, data, targets = None, transform=None, target_transform=None): # valori di default\n",
        "        self.imgs = data\n",
        "        self.targets = targets\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.imgs[idx]\n",
        "        if isinstance(img, str):\n",
        "          image = read_image(img)\n",
        "        else:\n",
        "          image = Image.fromarray(img.astype('uint8'), 'RGB')\n",
        "        if self.transform: # arriva qui con una PIL image\n",
        "            image1 = self.transform(image) # fa due trasformazioni\n",
        "            image2 = self.transform(image)\n",
        "        else:\n",
        "            image1 = image\n",
        "            image2 = image\n",
        "        return image1, image2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNH4IQc8Ccbb"
      },
      "outputs": [],
      "source": [
        "class Identity(torch.nn.Module):\n",
        "  def forward(self, x):\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2AhbTZhBtUJ",
        "outputId": "44bf1a1a-168e-4af4-e26f-ab9f9ae1e010"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "batch 0 loss 4.0882887840271\n",
            "\n",
            "batch 1 loss 1.4373500347137451\n",
            "\n",
            "batch 2 loss 1.4093679189682007\n",
            "\n",
            "batch 3 loss 1.4153363704681396\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True) # dataset\n",
        "\n",
        "color_jitter = transforms.ColorJitter(0.8, 0.8, 0.8, 0.2) # si distorce il colore con una certa probabilità tutti i canali\n",
        "\n",
        "\n",
        "transform = transforms.Compose([transforms.RandomResizedCrop(size=32),\n",
        "                                  transforms.RandomHorizontalFlip(),\n",
        "                                  transforms.RandomApply([color_jitter], p=0.8),\n",
        "                                  transforms.RandomGrayscale(p=0.2),\n",
        "                                  transforms.GaussianBlur(kernel_size=int(0.1 * 32)),\n",
        "                                  transforms.ToTensor()])\n",
        "\n",
        "trainset = CustomImageDataset(data.data, transform = transform)\n",
        "dataloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "byol = BYOL(models.resnet18())\n",
        "\n",
        "online_net_params = {'params': byol.online_net.parameters()}\n",
        "online_project_params = {'params': byol.online_projector.parameters()}\n",
        "\n",
        "optimizer = torch.optim.SGD([online_net_params, online_project_params], lr = 0.4, momentum = 0.9, weight_decay=1e-04)\n",
        "\n",
        "# qui c'è un doppio aggiornamento: aggiornamento SGD e aggiornamento EMA\n",
        "# EMA deve andare dopo SGD: prima si aggiorna il modello student e poi il target. Chiaramente il target deve aggiornarsi anche sullo stato attuale dello student\n",
        "\n",
        "for idx, (v1, v2) in enumerate(dataloader):\n",
        "    optimizer.zero_grad()\n",
        "    loss = byol(v1, v2) # gli diamo in ingresso le due view\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step() # sgd for online net\n",
        "    byol.update_moving_average() # ema for target net\n",
        "\n",
        "\n",
        "    print(f\"batch {idx} loss {loss.item()}\")\n",
        "    print()\n",
        "\n",
        "    if idx == 3:\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
