{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg7BGBFjv5SP"
      },
      "source": [
        "# Siamese Network\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "In this session, we are going to implement a Siamese Network.\n",
        "\n",
        "It takes as input two augmented versions of the same image and produces as output two feature vectors one for each version of the image.\n",
        "\n",
        "For simplicity, we will use the same backbone to process the views as in SimCLR paper.\n",
        "\n",
        "Andiamo a costruire una Siamese Architecture. Abbiamo un encoder che processa allo stesso tempo sia una immagine augmented che l'altra versione. Per ogni forward di una SN si va a fare due forward dell'encoder: uno che processa l'immagine e uno che processa l'altra.\n",
        "\n",
        "Si prende come backbone una resnet18 e andare a costruire una siamese network. Vogliamo implementarne il costruttore e il forward. Il costruttore dovrà inizializzare due versioni dell'encoder e il forward, prese x1 e x2 che sono le view1 e view2, restituisce in uscita le feature di queste immagini.\n",
        "\n",
        "(il costruttore dovrà inizializzare due versione dell'encoder, (le view sono x1 e x2) e devono fare il forward di queste immagini e devono restituire in uscita...)\n",
        "\n",
        "Ci fermiamo alle y lasciando stare l'MLP. Quindi prendiamo le view xi e xj, le processo con f() e ottengo yi e yj (negli appunti hi e hj).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wnoJ4nz6t3rN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "#import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G44sBdg5xSA3",
        "outputId": "fae3e029-c800-49c7-8b2a-6054abda2c5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Identity()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# you can use a resnet18 as backbone\n",
        "backbone = models.resnet18()\n",
        "#print(backbone)\n",
        "\n",
        "#! remember to delete the fc layer (we need just the CNN layers + flatten)\n",
        "backbone.fc = nn.Identity() # crea un fc finto che fa Identity(), la backbone così non avrà più come uscita un linear layer (che è fc), bensì avrà un layer identità che non fa nulla\n",
        "\n",
        "# Identity() prende x e ritorna x, non fa nulla! Lo facciamo perché sennò si rompe il forward\n",
        "\n",
        "print(backbone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Prp858FU_YaQ"
      },
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, data, targets = None, transform=None, target_transform=None): # valori di default\n",
        "        self.imgs = data\n",
        "        self.targets = targets\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.imgs[idx]\n",
        "        if isinstance(img, str):\n",
        "          image = read_image(img)\n",
        "        else:\n",
        "          image = Image.fromarray(img.astype('uint8'), 'RGB')\n",
        "        if self.transform: # arriva qui con una PIL image\n",
        "            image1 = self.transform(image) # fa due trasformazioni\n",
        "            image2 = self.transform(image)\n",
        "        else:\n",
        "            image1 = image\n",
        "            image2 = image\n",
        "        return image1, image2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Abbiamo due versioni di Siamese Network. \n",
        "1) c'è una simmetria della rete, lo stesso encoder è utilizzato in tutti i due branch della SN. \n",
        "2) c'è asimmetria tra le due reti, si hanno due encoder diversi (encoder1 con una rete ed encoder2 con un'altra rete) - si duplica il codice -\n",
        "\n",
        "Vediamo la versione più generale con due encoder diversi tra i due branch della rete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LEu_h8dhx8Lz"
      },
      "outputs": [],
      "source": [
        "class Identity(torch.nn.Module):\n",
        "  def forward(self, x):\n",
        "    return x\n",
        "\n",
        "\n",
        "class SiameseNet(nn.Module):\n",
        "    def __init__(self, backbone): # si prende in input la backbone (può essere la ResNet18 per esempio) e istanzio due versioni dell'encoder (encoder1 ed encoder2)\n",
        "        super().__init__()\n",
        "        self.encoder1 = backbone\n",
        "        # replace the fc layer with the identity layer\n",
        "        self.encoder1.fc = Identity() # equal to nn.Identity() # poi si fa il replace del fc con Identity()\n",
        "\n",
        "        self.encoder2 = backbone\n",
        "        self.encoder2.fc = Identity() # equal to nn.Identity()\n",
        "\n",
        "        # or, versione con un encoder\n",
        "        # self.encoder = backbone\n",
        "        # self.encoder.fc = Identity() # equal to nn.Identity()\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2, return_dict = True): # il forward prende in input le due view. qui si chiama il forward dell'encoder1 e dell'encoder2\n",
        "        x1 = self.encoder1(x1)\n",
        "        x2 = self.encoder2(x2)\n",
        "        if return_dict:\n",
        "          return {'view1': x1, 'view2': x2,} # in input ho le immagini, in uscita avrà le feature estratte con l'encoder1 e l'encoder2\n",
        "\n",
        "        # versione con un solo encoder\n",
        "        # x1 = self.encoder(x1)\n",
        "        # x2 = self.encoder(x2)\n",
        "\n",
        "        # return x1, x2 #\n",
        "\n",
        "\n",
        "        # return {x1, x2} lista\n",
        "        # return torch.cat((x1, x2), dim = 0) # most preferred one, tensore con versioni augmented sono concatenate in verticale\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Check output shape\n",
        "#x1, x2 = SiameseNet(backbone)(torch.randn(5, 3, 32, 32), torch.randn(5, 3, 32, 32)).shape\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlyBgKKzyxyB"
      },
      "source": [
        "Let's now use the Dataset which creates the two augmented views for each image from the [past lab session](https://colab.research.google.com/drive/1NJwAFbRiD4MdwWf__6P2Lm0xYk_DNdVu?usp=sharing) and create a loop with forward pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJAgRkIzyUTq",
        "outputId": "99117b59-d0bd-4de3-9404-993531ca1393"
      },
      "outputs": [],
      "source": [
        "#trainset = ... # scorsa volta, due uscite\n",
        "\n",
        "data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True) # dataset\n",
        "\n",
        "color_jitter = transforms.ColorJitter(0.8, 0.8, 0.8, 0.2) # si distorce il colore con una certa probabilità tutti i canali\n",
        "\n",
        "\n",
        "transform = transforms.Compose([transforms.RandomResizedCrop(size=32),\n",
        "                                  transforms.RandomHorizontalFlip(),\n",
        "                                  transforms.RandomApply([color_jitter], p=0.8),\n",
        "                                  transforms.RandomGrayscale(p=0.2),\n",
        "                                  transforms.GaussianBlur(kernel_size=int(0.1 * 32)),\n",
        "                                  transforms.ToTensor()])\n",
        "\n",
        "trainset = CustomImageDataset(data.data, transform = transform)\n",
        "dataloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "model = SiameseNet(backbone)\n",
        "\n",
        "for idx, data in enumerate(dataloader):\n",
        "    view1, view2 = data\n",
        "    output = model(view1, view2)\n",
        "    print(f\"batch {idx}:\")\n",
        "    print(output['view1'].shape)\n",
        "    print(output['view2'].shape)\n",
        "    print()\n",
        "\n",
        "    if idx == 3:\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
